---
layout: post
published: true
mathjax: false
featured: false
comments: false
title: Keras
categories:
  - tensorflow
---
## GTSRB with Keras

In my last blog post I described how I built a model for traffic sign recognition with tensorflow. The data set contains of 43 different German traffic signs for example:

![Screenshot from 2018-07-27 12-22-22.png]({{site.baseurl}}/images/Screenshot%20from%202018-07-27%2012-22-22.png)

The training set contains approximately 39.000 images and the test set around 12.000 images.

To use the network in a browser application I wanted to "deploy" it via tensorflow.js! ([https://www.youtube.com/watch?v=656l4IfhM10](https://www.youtube.com/watch?v=656l4IfhM10))

The easiest way to use a pretrained model in tensorflow.js seems to be by using Keras (`pip install keras`). Keras is a high-level API which can use multiple backends with tensorflow and tensorflow.js being part of them.

Because I still don't have a GPU I am going to try to work with colab.research.google.com which provides the same convenience as the Kaggle notebooks coming with access to one GPU.

Data Set Source: [http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)

Inspirational blog entry: [keras tutorial](https://chsasank.github.io/keras-tutorial.html)

## Loading the data

```python
import requests
import os.path
import zipfile
import os


# Load the dataset
train_data_url = 'http://benchmark.ini.rub.de/Dataset/GTSRB-Training_fixed.zip'
test_data_url = 'http://benchmark.ini.rub.de/Dataset/GTSRB_Online-Test-Images.zip'

def maybe_download_file(url):
  # the name of the file
  local_filename = url.split('/')[-1]
  if os.path.isfile(local_filename):
    print('File already exists')
    return local_filename
  # NOTE the stream=True parameter
  r = requests.get(url, stream=True)
  with open(local_filename, 'wb') as f:
      for chunk in r.iter_content(chunk_size=1024): 
          if chunk: # filter out keep-alive new chunks
              f.write(chunk)
              #f.flush() commented by recommendation from J.F.Sebastian
  return local_filename
  

def extract_archive(file_name, target_dir):
  # only extract the zip if the target_dir doesn't exist
  if os.path.isfile(target_dir):
      print('Dir already exists')
      return target_dir
  with open(file_name, 'rb') as f:
      zf = zipfile.ZipFile(f)
      zf.extractall(target_dir)
  return target_dir

  
file_path = maybe_download_file(test_data_url)
extract_archive(file_path, 'test_data')

file_path = maybe_download_file(train_data_url)
extract_archive(file_path, 'train_data')

```


The code downloads the training and testing data sets and extracts them.

The following lists the files:

```python

def list_files(startpath, depth=3):
  for root, dirs, files in os.walk(startpath):
      level = root.replace(startpath, '').count(os.sep)
      if level <= depth:
        indent = ' ' * 4 * (level)
        print('{}{}/'.format(indent, os.path.basename(root)))
        subindent = ' ' * 4 * (level + 1)
        for f in files:
            print('{}{}'.format(subindent, f))
          

list_files('.', depth=3)
```

**Install requirements:**

```
!pip install keras
!pip install scikit-image
```

## Preprocessing the images

```python

import numpy as np
import os
import glob

from skimage import io, transform, exposure, color

NUM_CLASSES = 43
IMG_SIZE = 32


def preprocess_img(img):
    # Histogram normalization in v channel
    hsv = color.rgb2hsv(img)
    hsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])
    img = color.hsv2rgb(hsv)

    # central square crop
    min_side = min(img.shape[:-1])
    centre = img.shape[0] // 2, img.shape[1] // 2
    img = img[centre[0] - min_side // 2:centre[0] + min_side // 2,
              centre[1] - min_side // 2:centre[1] + min_side // 2,
              :]

    # rescale to standard size
    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))

    # roll color axis to axis 0
    img = np.rollaxis(img, -1)

    return img


def get_class(img_path):
    return int(img_path.split('/')[-2])

root_dir = 'train_data/GTSRB/Training/'
imgs = []
labels = []

all_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))
np.random.shuffle(all_img_paths)

i = 0
for img_path in all_img_paths:
    if i%1000 == 0:
        print("{}".format(i))
    i += 1
    img = preprocess_img(io.imread(img_path))
    label = get_class(img_path)
    imgs.append(img)
    labels.append(label)

X = np.array(imgs, dtype='float32')
# Make one hot targets
Y = np.eye(NUM_CLASSES, dtype='uint8')[labels]

```


## A glimpse into the data set

```python
%matplotlib inline

# Visualize some images
n_images_per_class = 3
  
import random
import numpy as np
import matplotlib.pyplot as plt

w=32
h=32

fig=plt.figure(figsize=(10, 80))

columns = n_images_per_class
rows = NUM_CLASSES

print(X.shape)

for i in range(1, columns*rows +1):
  img = imgs[random.randint(0, 26640)]
  img = np.rollaxis(img, -1)
  img = np.rollaxis(img, -1)
  fig.add_subplot(rows, columns, i)
  plt.imshow(img)
    
plt.show()
```

![Screenshot from 2018-07-28 20-29-26.png]({{site.baseurl}}/images/Screenshot from 2018-07-28 20-29-26.png)


## Building a CNN with Keras

```
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.pooling import MaxPooling2D
from keras.optimizers import SGD
from keras import backend as K

print(K.tensorflow_backend._get_available_gpus())

K.set_image_data_format('channels_first')


def cnn_model():
    model = Sequential()

    model.add(Conv2D(32, (3, 3), padding='same',
                     input_shape=(3, IMG_SIZE, IMG_SIZE),
                     activation='relu'))
    model.add(Conv2D(32, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))

    model.add(Conv2D(64, (3, 3), padding='same',
                     activation='relu'))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))

    model.add(Conv2D(128, (3, 3), padding='same',
                     activation='relu'))
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(NUM_CLASSES, activation='softmax'))
    return model
```

## Training 

```
from keras.optimizers import SGD

model = cnn_model()

# let's train the model using SGD + momentum
lr = 0.01
sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

from keras.callbacks import LearningRateScheduler, ModelCheckpoint


def lr_schedule(epoch):
    return lr * (0.1 ** int(epoch / 10))

batch_size = 32
epochs = 30

model.fit(X, Y,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2,
          callbacks=[LearningRateScheduler(lr_schedule),
                     ModelCheckpoint('model.h5', save_best_only=True)]
          )


```


## Evaluate

```python

import pandas as pd

# train_data_labels/
#         GT-final_test.csv

# test_data/
#        GTSRB/
#            Readme-Images-Final-test.txt
#            Final_Test/
#                Images/
#                    GT-final_test.test.csv


test = pd.read_csv('train_data_labels/GT-final_test.csv', sep=';')

# Load test dataset
X_test = []
y_test = []
i = 0
for file_name, class_id in zip(list(test['Filename']), list(test['ClassId'])):
    img_path = os.path.join('test_data/GTSRB/Final_Test/Images/', file_name)
    X_test.append(preprocess_img(io.imread(img_path)))
    y_test.append(class_id)

X_test = np.array(X_test)
y_test = np.array(y_test)

# predict and evaluate
y_pred = model.predict_classes(X_test)
acc = np.sum(y_pred == y_test) / np.size(y_pred)
print("Test accuracy = {}".format(acc))
```

Test accuracy = 0.9661124307205067

Using TensorFlow backend.
['/job:localhost/replica:0/task:0/device:GPU:0']













 








